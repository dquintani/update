{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59455dd5-248c-4a30-83f0-997ca169ee5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T18:14:21.639195Z",
     "iopub.status.busy": "2022-08-26T18:14:21.638647Z",
     "iopub.status.idle": "2022-08-26T18:14:21.947003Z",
     "shell.execute_reply": "2022-08-26T18:14:21.946228Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please use the HuggingFace dataset library, or\n",
      "download from https://socialgrep.com/datasets.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython import get_ipython\n",
    "\n",
    "# envs# coding=utf-8\n",
    "# Copyright 2020 The HuggingFace Datasets Authors and the current dataset script contributor.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"The SocialGrep dataset loader base.\"\"\"\n",
    "\n",
    "\n",
    "import csv\n",
    "import os\n",
    "\n",
    "import datasets\n",
    "\n",
    "\n",
    "DATASET_NAME = \"the-reddit-climate-change-dataset\"\n",
    "DATASET_TITLE = \"the-reddit-climate-change-dataset\"\n",
    "\n",
    "DATASET_DESCRIPTION = \"\"\"\\\n",
    "All the mentions of climate change on Reddit before Sep 1 2022.\n",
    "\"\"\"\n",
    "\n",
    "_HOMEPAGE = f\"https://socialgrep.com/datasets/{DATASET_NAME}\"\n",
    "\n",
    "_LICENSE = \"CC-BY v4.0\"\n",
    "\n",
    "URL_TEMPLATE = \"https://exports.socialgrep.com/download/public/{dataset_file}.zip\"\n",
    "DATASET_FILE_TEMPLATE = \"{dataset}-{type}.csv\"\n",
    "\n",
    "_DATASET_FILES = {\n",
    "    'posts': DATASET_FILE_TEMPLATE.format(dataset=DATASET_NAME, type=\"posts\"),\n",
    "    'comments': DATASET_FILE_TEMPLATE.format(dataset=DATASET_NAME, type=\"comments\"),\n",
    "}\n",
    "\n",
    "_CITATION = f\"\"\"\\\n",
    "        @misc{{socialgrep:{DATASET_NAME},\n",
    "title = {{{DATASET_TITLE}}},\n",
    "author={{Lexyr Inc.\n",
    "}},\n",
    "year={{2022}}\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class theredditclimatechangedataset(datasets.GeneratorBasedBuilder):\n",
    "    VERSION = datasets.Version(\"1.0.0\")\n",
    "\n",
    "    # This is an example of a dataset with multiple configurations.\n",
    "    # If you don't want/need to define several sub-sets in your dataset,\n",
    "    # just remove the BUILDER_CONFIG_CLASS and the BUILDER_CONFIGS attributes.\n",
    "\n",
    "    # If you need to make complex sub-parts in the datasets with configurable options\n",
    "    # You can create your own builder configuration class to store attribute, inheriting from datasets.BuilderConfig\n",
    "    # BUILDER_CONFIG_CLASS = MyBuilderConfig\n",
    "\n",
    "    # You will be able to load one or the other configurations in the following list with\n",
    "    # data = datasets.load_dataset('my_dataset', 'first_domain')\n",
    "    # data = datasets.load_dataset('my_dataset', 'second_domain')\n",
    "    BUILDER_CONFIGS = [\n",
    "        datasets.BuilderConfig(name=\"posts\", version=VERSION, description=\"The dataset posts.\"),\n",
    "        datasets.BuilderConfig(name=\"comments\", version=VERSION, description=\"The dataset comments.\"),\n",
    "    ]\n",
    "\n",
    "    def _info(self):\n",
    "        if self.config.name == \"posts\":  # This is the name of the configuration selected in BUILDER_CONFIGS above\n",
    "            features = datasets.Features(\n",
    "                {\n",
    "                    \"type\": datasets.Value(\"string\"),\n",
    "                    \"id\": datasets.Value(\"string\"),\n",
    "                    \"subreddit.id\": datasets.Value(\"string\"),\n",
    "                    \"subreddit.name\": datasets.Value(\"string\"),\n",
    "                    \"subreddit.nsfw\": datasets.Value(\"bool\"),\n",
    "                    \"created_utc\": datasets.Value(\"timestamp[s,tz=utc]\"),\n",
    "                    \"permalink\": datasets.Value(\"string\"),\n",
    "                    \"domain\": datasets.Value(\"string\"),\n",
    "                    \"url\": datasets.Value(\"string\"),\n",
    "                    \"selftext\": datasets.Value(\"large_string\"),\n",
    "                    \"title\": datasets.Value(\"string\"),\n",
    "                    \"score\": datasets.Value(\"int32\"),\n",
    "                }\n",
    "            )\n",
    "        else:  # This is an example to show how to have different features for \"first_domain\" and \"second_domain\"\n",
    "            features = datasets.Features(\n",
    "                {\n",
    "                    \"type\": datasets.ClassLabel(num_classes=2, names=['post', 'comment']),\n",
    "                    \"id\": datasets.Value(\"string\"),\n",
    "                    \"subreddit.id\": datasets.Value(\"string\"),\n",
    "                    \"subreddit.name\": datasets.Value(\"string\"),\n",
    "                    \"subreddit.nsfw\": datasets.Value(\"bool\"),\n",
    "                    \"created_utc\": datasets.Value(\"timestamp[s,tz=utc]\"),\n",
    "                    \"permalink\": datasets.Value(\"string\"),\n",
    "                    \"body\": datasets.Value(\"large_string\"),\n",
    "                    \"sentiment\": datasets.Value(\"float32\"),\n",
    "                    \"score\": datasets.Value(\"int32\"),\n",
    "                }\n",
    "            )\n",
    "        return datasets.DatasetInfo(\n",
    "            # This is the description that will appear on the datasets page.\n",
    "            description=DATASET_DESCRIPTION,\n",
    "            # This defines the different columns of the dataset and their types\n",
    "            features=features,  # Here we define them above because they are different between the two configurations\n",
    "            # If there's a common (input, target) tuple from the features,\n",
    "            # specify them here. They'll be used if as_supervised=True in\n",
    "            # builder.as_dataset.\n",
    "            supervised_keys=None,\n",
    "            # Homepage of the dataset for documentation\n",
    "            homepage=_HOMEPAGE,\n",
    "            # License for the dataset if available\n",
    "            license=_LICENSE,\n",
    "            # Citation for the dataset\n",
    "            citation=_CITATION,\n",
    "        )\n",
    "\n",
    "    def _split_generators(self, dl_manager):\n",
    "        \"\"\"Returns SplitGenerators.\"\"\"\n",
    "        # If several configurations are possible (listed in BUILDER_CONFIGS), the configuration selected by the user is in self.config.name\n",
    "\n",
    "        # dl_manager is a datasets.download.DownloadManager that can be used to download and extract URLs\n",
    "        # It can accept any type or nested list/dict and will give back the same structure with the url replaced with path to local files.\n",
    "        # By default the archives will be extracted and a path to a cached folder where they are extracted is returned instead of the archive\n",
    "        my_urls = [URL_TEMPLATE.format(dataset_file=_DATASET_FILES[self.config.name])]\n",
    "        data_dir = dl_manager.download_and_extract(my_urls)[0]\n",
    "        return [\n",
    "            datasets.SplitGenerator(\n",
    "                name=datasets.Split.TRAIN,\n",
    "                # These kwargs will be passed to _generate_examples\n",
    "                gen_kwargs={\n",
    "                    \"filepath\": os.path.join(data_dir, _DATASET_FILES[self.config.name]),\n",
    "                    \"split\": \"train\",\n",
    "                },\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def _generate_examples(\n",
    "        self, filepath, split  # method parameters are unpacked from `gen_kwargs` as given in `_split_generators`\n",
    "    ):\n",
    "        \"\"\" Yields examples as (key, example) tuples. \"\"\"\n",
    "        # This method handles input defined in _split_generators to yield (key, example) tuples from the dataset.\n",
    "        bool_cols = [\"subreddit.nsfw\"]\n",
    "        int_cols = [\"score\", \"created_utc\"]\n",
    "        float_cols = [\"sentiment\"]\n",
    "\n",
    "        with open(filepath, encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                for col in bool_cols:\n",
    "                    if col in row:\n",
    "                        if row[col]:\n",
    "                            row[col] = (row[col] == \"true\")\n",
    "                        else:\n",
    "                            row[col] = None\n",
    "                for col in int_cols:\n",
    "                    if col in row:\n",
    "                        if row[col]:\n",
    "                            row[col] = int(row[col])\n",
    "                        else:\n",
    "                            row[col] = None\n",
    "                for col in float_cols:\n",
    "                    if col in row:\n",
    "                        if row[col]:\n",
    "                            row[col] = float(row[col])\n",
    "                        else:\n",
    "                            row[col] = None\n",
    "\n",
    "                if row[\"type\"] == \"post\":\n",
    "                    key = f\"t3_{row['id']}\"\n",
    "                if row[\"type\"] == \"comment\":\n",
    "                    key = f\"t1_{row['id']}\"\n",
    "                yield key, row\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Please use the HuggingFace dataset library, or\")\n",
    "    print(\"download from https://socialgrep.com/datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d762cd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T18:14:21.950713Z",
     "iopub.status.busy": "2022-08-26T18:14:21.950061Z",
     "iopub.status.idle": "2022-08-26T18:14:21.956213Z",
     "shell.execute_reply": "2022-08-26T18:14:21.955631Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/daqub/Documents/GITHUB-UPDATE/update'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb06dfbc-d234-4190-a4bb-dc6c81394684",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T18:14:21.959160Z",
     "iopub.status.busy": "2022-08-26T18:14:21.958932Z",
     "iopub.status.idle": "2022-08-26T18:14:21.985689Z",
     "shell.execute_reply": "2022-08-26T18:14:21.985058Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to check:  BoliGrafica/_bots/twitter_covid_replies/covid_reply.ipynb\n",
      "previous log found: 2022-09-18 18:44\n",
      "difference in execution is: 0.0471 days\n",
      "needed difference to execute: 2 days\n",
      "Executtion:  False ... DID NOT EXECUTE NOR LOG\n",
      "Finished:  BoliGrafica/_bots/twitter_covid_replies/covid_reply.ipynb \n",
      "\n",
      "ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ NEXT ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹\n",
      "\n",
      "\n",
      "starting to check:  BoliGrafica/_bots/twitter_bo-indicadores/SDSN/sdsn_map_post.ipynb\n",
      "previous log found: 2022-09-18 12:30\n",
      "difference in execution is: 0.3068 days\n",
      "needed difference to execute: 0.625 days\n",
      "Executtion:  False ... DID NOT EXECUTE NOR LOG\n",
      "Finished:  BoliGrafica/_bots/twitter_bo-indicadores/SDSN/sdsn_map_post.ipynb \n",
      "\n",
      "ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ NEXT ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹\n",
      "\n",
      "\n",
      "starting to check:  BoliGrafica/_bots/twitter_bo-indicadores/Banco_Mundial_World_Bank/wbgapi_indicator_post.ipynb\n",
      "previous log found: 2022-09-18 12:30\n",
      "difference in execution is: 0.3068 days\n",
      "needed difference to execute: 0.5833 days\n",
      "Executtion:  False ... DID NOT EXECUTE NOR LOG\n",
      "Finished:  BoliGrafica/_bots/twitter_bo-indicadores/Banco_Mundial_World_Bank/wbgapi_indicator_post.ipynb \n",
      "\n",
      "ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ NEXT ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹\n",
      "\n",
      "\n",
      "starting to check:  BoliGrafica/monkeypox_viruela_simica/monkeypox_post.ipynb\n",
      "previous log found: 2022-09-18 19:46\n",
      "difference in execution is: 0.004 days\n",
      "needed difference to execute: 6 days\n",
      "Executtion:  False ... DID NOT EXECUTE NOR LOG\n",
      "Finished:  BoliGrafica/monkeypox_viruela_simica/monkeypox_post.ipynb \n",
      "\n",
      "ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ NEXT ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹\n",
      "\n",
      "\n",
      "starting to check:  Greenhouse_Data/_bots/twitter_country_post/country_of_the_day.ipynb\n",
      "previous log found: 2022-09-18 18:47\n",
      "difference in execution is: 0.045 days\n",
      "needed difference to execute: 1 days\n",
      "Executtion:  False ... DID NOT EXECUTE NOR LOG\n",
      "Finished:  Greenhouse_Data/_bots/twitter_country_post/country_of_the_day.ipynb \n",
      "\n",
      "ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ NEXT ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹\n",
      "\n",
      "\n",
      "starting to check:  Greenhouse_Data/_bots/twitter_greenhouse_summon/greenhouse_summon.ipynb\n",
      "previous log found: 2022-09-18 18:45\n",
      "difference in execution is: 0.0464 days\n",
      "needed difference to execute: 0.25 days\n",
      "Executtion:  False ... DID NOT EXECUTE NOR LOG\n",
      "Finished:  Greenhouse_Data/_bots/twitter_greenhouse_summon/greenhouse_summon.ipynb \n",
      "\n",
      "ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ NEXT ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹\n",
      "\n",
      "\n",
      "starting to check:  Greenhouse_Data/random_fig/random_fig.ipynb\n",
      "previous log found: 2022-09-17 22:16\n",
      "difference in execution is: 0.8999 days\n",
      "needed difference to execute: 1 days\n",
      "Executtion:  False ... DID NOT EXECUTE NOR LOG\n",
      "Finished:  Greenhouse_Data/random_fig/random_fig.ipynb \n",
      "\n",
      "ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ NEXT ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹\n",
      "\n",
      "\n",
      "starting to check:  Greenhouse_Data/NOAA_updates/NOAA_updates.ipynb\n",
      "previous log found: 2022-09-17 16:23\n",
      "difference in execution is: 1.145 days\n",
      "needed difference to execute: 2 days\n",
      "Executtion:  False ... DID NOT EXECUTE NOR LOG\n",
      "Finished:  Greenhouse_Data/NOAA_updates/NOAA_updates.ipynb \n",
      "\n",
      "ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ NEXT ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹\n",
      "\n",
      "\n",
      "ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»\n",
      "               FINITO\n",
      "ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»\n"
     ]
    }
   ],
   "source": [
    "def SCHEDULED_RUNNER(notebook_path, schedule_in_days = None):\n",
    "    \n",
    "    print(\"starting to check: \", notebook_path)\n",
    "    \n",
    "\n",
    "\n",
    "    # if there is no schedule, run immediately\n",
    "    if schedule_in_days == None:\n",
    "        EXECUTE = True\n",
    "        \n",
    "\n",
    "\n",
    "    # else run on a schedule based on last execution time\n",
    "    else:\n",
    "        # check log\n",
    "        log = pd.read_csv(\"log.csv\", index_col=0)\n",
    "        try:\n",
    "            last_executed = log.loc[notebook_path][\"last_executed\"]\n",
    "            print(f\"previous log found: {last_executed}\")\n",
    "        except:\n",
    "            print(\"no log found (first time?) - creating dummy entry (year 2000)\")\n",
    "            log.loc[notebook_path] = [\"2002-07-16 18:32\",np.nan]\n",
    "            \n",
    "            # try again\n",
    "            last_executed = log.loc[notebook_path][\"last_executed\"]\n",
    "            \n",
    "        # get time difference in days\n",
    "        import datetime\n",
    "        now = datetime.datetime.now()\n",
    "        last_executed = datetime.datetime.strptime(last_executed, '%Y-%m-%d %H:%M')\n",
    "        diff_seconds = now - last_executed\n",
    "        diff_days = diff_seconds.total_seconds() / 60 /60 /24\n",
    "        print(f\"difference in execution is: {round(diff_days,4)} days\")\n",
    "        print(f\"needed difference to execute: {round(schedule_in_days,4)} days\")\n",
    "        \n",
    "        # check diff\n",
    "        if diff_days > schedule_in_days:\n",
    "            EXECUTE = True\n",
    "        else:\n",
    "            EXECUTE = False\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # EXECUTE?\n",
    "\n",
    "\n",
    "    # EXECUTE TRUE\n",
    "    if EXECUTE == True:\n",
    "        print(\"Executtion: \", str(EXECUTE), \"... STARTING NOW\")\n",
    "\n",
    "        # go to folder to run script locally\n",
    "        folder_path = notebook_path.split(\"/\")[:-1]\n",
    "        os.chdir(\"/\".join(folder_path))\n",
    "\n",
    "        # try to run script, if it fails, it fails\n",
    "        try:\n",
    "            get_ipython().run_line_magic(\"run\", notebook_path.split(\"/\")[-1])\n",
    "            status = \"... t'was a SUCCESS! :)\"\n",
    "            print(status)\n",
    "        except:\n",
    "            status = \"... t'was a FAILURE. :(\"\n",
    "            print(status)\n",
    "\n",
    "        # go back to root repository\n",
    "        os.chdir(\"\".join([\"../\"] * len(folder_path)))\n",
    "        \n",
    "        \n",
    "        # log\n",
    "        log.loc[notebook_path] = [datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\"), status]\n",
    "        log.to_csv(\"log.csv\")\n",
    "        print(\"New log entry successful!\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # EXECUTE FALSE\n",
    "    elif EXECUTE == False:\n",
    "        print(\"Executtion: \", str(EXECUTE), \"... DID NOT EXECUTE NOR LOG\")\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Finished: \", notebook_path, \"\\n\\nğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ NEXT ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹ğŸ˜‹\\n\\n\")\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################\n",
    "###################### SCHEDULE #######################\n",
    "########################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ##### BOLIGRAFICA #####\n",
    "SCHEDULED_RUNNER(notebook_path = \"BoliGrafica/_bots/twitter_covid_replies/covid_reply.ipynb\", schedule_in_days= 2)\n",
    "SCHEDULED_RUNNER(notebook_path = \"BoliGrafica/_bots/twitter_bo-indicadores/SDSN/sdsn_map_post.ipynb\", schedule_in_days=15/24)\n",
    "SCHEDULED_RUNNER(notebook_path = \"BoliGrafica/_bots/twitter_bo-indicadores/Banco_Mundial_World_Bank/wbgapi_indicator_post.ipynb\", schedule_in_days=14/24)\n",
    "SCHEDULED_RUNNER(notebook_path = \"BoliGrafica/monkeypox_viruela_simica/monkeypox_post.ipynb\", schedule_in_days=6)\n",
    "\n",
    "###### GREENHOUSE DATA #####\n",
    "SCHEDULED_RUNNER(notebook_path = \"Greenhouse_Data/_bots/twitter_country_post/country_of_the_day.ipynb\", schedule_in_days = 1)    \n",
    "SCHEDULED_RUNNER(notebook_path = \"Greenhouse_Data/_bots/twitter_greenhouse_summon/greenhouse_summon.ipynb\", schedule_in_days = 6/24)    \n",
    "SCHEDULED_RUNNER(notebook_path = \"Greenhouse_Data/random_fig/random_fig.ipynb\", schedule_in_days = 1)    \n",
    "SCHEDULED_RUNNER(notebook_path = \"Greenhouse_Data/NOAA_updates/NOAA_updates.ipynb\", schedule_in_days = 2)\n",
    "\n",
    "\n",
    "print(\"ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»\\n               FINITO\\nğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»ğŸ»\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85300601-1e1f-4f1c-8b80-bb3d16470da4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be59bc6-1940-45e1-9e81-478b87935bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3879f593f72178d4cdaa5d992027cc0eafe2075e6c7e6c2da0f0dd376bbd7072"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
